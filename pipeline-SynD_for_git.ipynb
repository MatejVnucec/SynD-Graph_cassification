{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c63975-6740-4a7f-9052-668614a78843",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install pytorch=1.12 torchvision torchaudio cudatoolkit=11.3 -c pytorch -y -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc6d422-e959-492e-a232-0b9b78bb6bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fd8589-a39b-44b6-ac1c-ab01d896b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install -c conda-forge pyts -q -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b981078-d167-4691-b098-bcfcce2e6a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llvmpy\n",
    "!pip install cython\n",
    "!pip install numba\n",
    "!pip install pandas\n",
    "!pip install networkx\n",
    "!pip install matplotlib\n",
    "!pip install ts2vg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca3d551-0aab-49a3-8fb9-2b5632323daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "import networkx as nx\n",
    "import ts2vg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from pyts.image import MarkovTransitionField\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "from matplotlib import pyplot\n",
    "from ts2vg import NaturalVG\n",
    "from ts2vg import HorizontalVG\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a897315-6631-47de-b119-1dbeb9324cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#usualy there was a sample with length not equal to the others, that is why we need X_repair\n",
    "def anomaly_finder():\n",
    "    anomaly = []\n",
    "    X_temp = np.load(dataFileName, mmap_mode=None, allow_pickle=True)\n",
    "    for i in range(len(X_temp)):\n",
    "        if len(X_temp[i]) != len(X_temp[0]):\n",
    "            anomaly = np.append(anomaly, i)\n",
    "    return anomaly\n",
    "\n",
    "# converts words to numbers\n",
    "def anomaly_numeric(Y):\n",
    "    labels = np.empty([len(Y)-1,1], dtype=int)\n",
    "    t = Y[0]\n",
    "    k = 0\n",
    "    count = 0\n",
    "    for i in range(len(Y)):\n",
    "        if i not in hidden_anomaly:#hidden_anomaly: #76407,39617,21019,11832\n",
    "            if Y[i] != t:\n",
    "                count += 1\n",
    "                t = Y[i]\n",
    "            labels[i-k] = count\n",
    "        else:\n",
    "            k = 1\n",
    "            \n",
    "    return labels.reshape(1,-1)[0]\n",
    "\n",
    "# maps every utility number and number location in anomaly_numeric()\n",
    "def location_of_labels(Y_temp):\n",
    "    range_labels = np.empty([max_samp,4], dtype='<U21')\n",
    "    t = 'n'\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    count3 = 0\n",
    "    k = 0\n",
    "    for i in range(len(Y_temp)):\n",
    "        if i not in hidden_anomaly:#hidden_anomaly: #76407,39617,21019,11832\n",
    "            count3 += 1\n",
    "            if count1 != 0:\n",
    "                #length of samples for a given utility\n",
    "                range_labels[count1-1][2] = count3\n",
    "            \n",
    "            if t != Y_temp[i]:\n",
    "                \n",
    "                #sequence number\n",
    "                range_labels[count1][0] = count1\n",
    "                \n",
    "                #name of utility per sequence number\n",
    "                range_labels[count1][1] = Y_temp[i]\n",
    "                \n",
    "                #where the sequence number represents the given utility samples\n",
    "                range_labels[count1][3] = count2\n",
    "                \n",
    "                t = Y_temp[i]\n",
    "                count1 += 1\n",
    "                count3 = 0\n",
    "            count2 += 1\n",
    "\n",
    "    return range_labels\n",
    "\n",
    "#Fix X weird shape\n",
    "def X_repair(X_temp):\n",
    "    X = np.empty([len(X_temp)-1,len(X_temp[0])], dtype='float64')\n",
    "    k = 0\n",
    "    for i in range(len(X_temp)):\n",
    "        if i not in hidden_anomaly: #hidden_anomaly: #76407,39617,21019,11832\n",
    "            for j in range(len(X[0])):\n",
    "                X[i-k][j] = X_temp[i][j]\n",
    "        else:\n",
    "            k = k + 1 \n",
    "    return X\n",
    "\n",
    "#reduce length so all utilities have the same number of samples\n",
    "def X_reduce_length_to_shortest(X,lol,cut):\n",
    "    \n",
    "    length_X = (lol[:,2].astype('int').min()-1) * cut\n",
    "    \n",
    "    dimensions = 0\n",
    "    for i in range(len(lol)):\n",
    "        if lol[i][2].astype('int') > length_X:\n",
    "            dimensions += length_X\n",
    "        else:\n",
    "            dimensions += lol[i][2].astype('int')\n",
    "    \n",
    "    X_new = np.empty([dimensions,len(X[0])], dtype='float64')\n",
    "    Y = np.empty([dimensions,1], dtype=int)\n",
    "    \n",
    "    temp_len = 0\n",
    "    temp_temp = 0\n",
    "    temp = 0\n",
    "    for i in range(len(lol)):\n",
    "        for j in range(length_X):\n",
    "            if lol[i][2].astype('int') > j:\n",
    "                Y[temp_temp + j] = i\n",
    "                X_new[temp_temp + j] = X[j + temp]\n",
    "                temp_len = j+1\n",
    "    \n",
    "        temp = temp + lol[i][2].astype('int')\n",
    "        lol[i][2] = temp_len\n",
    "        lol[i][3] = temp_temp\n",
    "        temp_temp = temp_temp + temp_len\n",
    "        temp_len = 0\n",
    "        \n",
    "    return X_new, Y.reshape(1,-1)[0], lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1507f595-2818-4106-bfa8-6c2b85d1668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the graph \n",
    "def create_MTF_graph():\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    global lol, hidden_anomaly, max_samp\n",
    "    \n",
    "    hidden_anomaly = anomaly_finder()\n",
    "    \n",
    "    Y_temp = np.load(labelsFileName, mmap_mode=None, allow_pickle=False)\n",
    "    Y = anomaly_numeric(Y_temp)\n",
    "    \n",
    "    #for visualisation of what is where in the dataset\n",
    "    max_samp = Y.max()+1\n",
    "    lol = location_of_labels(Y_temp)\n",
    "    \n",
    "    X_temp = np.load(dataFileName, mmap_mode=None, allow_pickle=True)\n",
    "    X = X_repair(X_temp)\n",
    "    \n",
    "    if cut != 0:\n",
    "        X, Y, lol = X_reduce_length_to_shortest(X,lol,cut)\n",
    "        \n",
    "    MTF = MarkovTransitionField(n_bins = n_bins)\n",
    "    X_gaf = MTF.fit_transform(X)\n",
    "    output = []\n",
    "    \n",
    "    from sklearn.utils import class_weight\n",
    "    global class_weights\n",
    "    class_weights = torch.tensor(class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                                   classes=np.unique(Y),\n",
    "                                                                   y=Y))\n",
    "    def adjToEdgidx(adj_mat):\n",
    "        edge_index = torch.from_numpy(adj_mat).nonzero().t().contiguous()\n",
    "        row, col = edge_index\n",
    "        edge_weight = adj_mat[row, col]#adj_mat[row, col]\n",
    "        return edge_index, edge_weight\n",
    "    \n",
    "    for i, j in enumerate(X_gaf):\n",
    "        edge_index, edge_weight = adjToEdgidx(j)\n",
    "        #Into Data save node values \"x\", edge index from adjacency matrix and edge features/attributes, finally labels       \n",
    "        output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), \n",
    "                           edge_index=edge_index, \n",
    "                           edge_attr=torch.unsqueeze(torch.tensor(edge_weight, dtype=torch.double),1), \n",
    "                           y=torch.tensor(Y[i], dtype=torch.long)))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950720ae-c36d-49a6-bd2a-c25bf2279c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the visual graph \n",
    "def create_visual_graph():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    global lol, hidden_anomaly, max_samp\n",
    "    \n",
    "    hidden_anomaly = anomaly_finder()\n",
    "    \n",
    "    Y_temp = np.load(labelsFileName, mmap_mode=None, allow_pickle=False)\n",
    "    Y = anomaly_numeric(Y_temp)\n",
    "    \n",
    "    #for visualisation of what is where in the dataset\n",
    "    max_samp = Y.max()+1\n",
    "    lol = location_of_labels(Y_temp)\n",
    "    \n",
    "    X_temp = np.load(dataFileName, mmap_mode=None, allow_pickle=True)\n",
    "    X = X_repair(X_temp)\n",
    "    \n",
    "    if cut != 0:\n",
    "        X, Y, lol = X_reduce_length_to_shortest(X,lol,cut)\n",
    "        \n",
    "    output = []\n",
    "    \n",
    "    from sklearn.utils import class_weight\n",
    "    global class_weights\n",
    "    class_weights = torch.tensor(class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                                   classes=np.unique(Y),\n",
    "                                                                   y=Y))\n",
    "\n",
    "    def adjToEdgidx(X_current):\n",
    "        g = NaturalVG(weighted='distance')\n",
    "        # g = HorizontalVG(weighted='distance')\n",
    "        g.build(X_current)\n",
    "\n",
    "        adj_mat_visual = np.zeros([len(X_current),len(X_current)], dtype='float')\n",
    "        for i in range(len(g.edges)):\n",
    "            x, y, q =g.edges[i]\n",
    "            adj_mat_visual[x,y] = q#/g.weights.max()\n",
    "            adj_mat_visual[y,x] = q#/g.weights.max()\n",
    "        \n",
    "        edge_index = torch.from_numpy(adj_mat_visual).nonzero().t().contiguous()\n",
    "        row, col = edge_index\n",
    "        edge_weight = adj_mat_visual[row, col]\n",
    "        \n",
    "        # edge_weight = np.reshape(edge_weight,(len(edge_weight),2))\n",
    "        \n",
    "        return edge_index, edge_weight\n",
    "    \n",
    "#     for i in range(len(X)):\n",
    "#         g = NaturalVG(weighted='distance')\n",
    "#         #g = HorizontalVG(weighted='distance')\n",
    "#         g.build(X[i])\n",
    "\n",
    "#         edges = np.array(g.edges)\n",
    "#         edge_index = np.reshape(edges[:,:2],(2,len(edges)))\n",
    "        \n",
    "#         weights = np.array(g.weights)\n",
    "#         edge_attr = (weights/weights.max())\n",
    "        \n",
    "    for i in range(len(X)):\n",
    "        edge_index, edge_weight = adjToEdgidx(X[i])\n",
    "        output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), \n",
    "                           edge_index=torch.tensor(edge_index, dtype=torch.int64), \n",
    "                           edge_attr=torch.unsqueeze(torch.tensor(edge_weight, dtype=torch.double),1),\n",
    "                           y=torch.tensor(Y[i], dtype=torch.long) ))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af5e176-9092-4beb-9bf9-e00bd8ea9d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the graph \n",
    "def create_join_graph():\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")    \n",
    "    global lol, hidden_anomaly, max_samp\n",
    "    \n",
    "    #checks if there are any shapes that do not fit others\n",
    "    hidden_anomaly = anomaly_finder()\n",
    "    \n",
    "    # preparation for creating a graph\n",
    "    Y_temp = np.load(labelsFileName, mmap_mode=None, allow_pickle=False)\n",
    "    Y = anomaly_numeric(Y_temp)\n",
    "    \n",
    "    #for visualisation of what is where in the dataset\n",
    "    max_samp = Y.max()+1\n",
    "    lol = location_of_labels(Y_temp)\n",
    "    \n",
    "    X_temp = np.load(dataFileName, mmap_mode=None, allow_pickle=True)\n",
    "    X = X_repair(X_temp)\n",
    "    if cut != 0:\n",
    "        X, Y, lol = X_reduce_length_to_shortest(X,lol,cut)\n",
    "    \n",
    "    MTF = MarkovTransitionField(n_bins = n_bins)\n",
    "    X_gaf = MTF.fit_transform(X)\n",
    "    \n",
    "    output= []\n",
    "\n",
    "    global class_weights\n",
    "    class_weights = torch.tensor(class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                                   classes=np.unique(Y),\n",
    "                                                                   y=Y))\n",
    "        \n",
    "    def adjToEdgidx(adj_mat_MTF,X_current):\n",
    "        g = NaturalVG(weighted='distance')\n",
    "        #g = HorizontalVG(weighted='distance')\n",
    "        g.build(X_current)\n",
    "        \n",
    "        #create matrix for visual\n",
    "        adj_mat_visual = np.zeros([len(adj_mat_MTF),len(adj_mat_MTF)], dtype='float')\n",
    "        for i in range(len(g.edges)):\n",
    "            x, y, q =g.edges[i]\n",
    "            adj_mat_visual[x,y] = q#/g.weights.max()\n",
    "            adj_mat_visual[y,x] = q#/g.weights.max()\n",
    "        \n",
    "        edge_index = torch.from_numpy(adj_mat_MTF).nonzero().t().contiguous()\n",
    "        \n",
    "        #join two edge_weight arrays (visual is converted to fit MTF) \n",
    "        row, col = edge_index\n",
    "        edge_weight = np.zeros([len(row),2], dtype='float')\n",
    "        edge_weight[:,0] = adj_mat_MTF[row, col]\n",
    "        edge_weight[:,1] = adj_mat_visual[row, col]\n",
    "        \n",
    "        # edge_weight = np.reshape(edge_weight,(len(edge_weight),2))\n",
    "        \n",
    "        return edge_index, edge_weight\n",
    "    \n",
    "    for i, j in enumerate(X_gaf):\n",
    "        edge_index, edge_weight = adjToEdgidx(j,X[i])\n",
    "        #Into Data save node values \"x\", edge index from adjacency matrix and edge features/attributes, finally labels       \n",
    "        output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), \n",
    "                    edge_index=edge_index, \n",
    "                    edge_attr=torch.tensor(edge_weight, dtype=torch.double), \n",
    "                    y=torch.tensor(Y[i], dtype=torch.long)))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b197a-136c-4297-a08a-9994bf305136",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graph Clasification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa822eb2-ed3f-4db3-9bf8-c49434f15f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.CrossEntropyLoss):\n",
    "    ''' Focal loss for classification tasks on imbalanced datasets '''\n",
    "\n",
    "    def __init__(self, gamma=1, alpha=None, ignore_index=-100, reduction='mean'):\n",
    "        super().__init__(weight=alpha, ignore_index=ignore_index, reduction='none')\n",
    "        self.reduction = reduction\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, input_, target):\n",
    "        cross_entropy = super().forward(input_, target)\n",
    "        # Temporarily mask out ignore index to '0' for valid gather-indices input.\n",
    "        # This won't contribute final loss as the cross_entropy contribution\n",
    "        # for these would be zero.\n",
    "        target = target * (target != self.ignore_index).long()\n",
    "        input_prob = torch.gather(F.softmax(input_, 1), 1, target.unsqueeze(1))\n",
    "        loss = torch.pow(1 - input_prob, self.gamma) * cross_entropy\n",
    "        \n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(loss) \n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(loss) \n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a01a6d5-aa80-4aae-b121-6486c5e5f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, CrossEntropyLoss\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool, global_max_pool, ChebConv, global_sort_pool\n",
    "from torch.nn import Sequential, BatchNorm1d, ReLU, Dropout\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GINConv, GINEConv, GATv2Conv\n",
    "\n",
    "\n",
    "class GINE(torch.nn.Module):\n",
    "    \"\"\"GIN\"\"\"\n",
    "    def __init__(self, dim_h):\n",
    "        super(GINE, self).__init__()\n",
    "        edge_dim = 1\n",
    "        \n",
    "        \n",
    "        self.conv1 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h),\n",
    "                       BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv2 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv3 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv4 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv5 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "    \n",
    "        self.lin1 = Linear(dim_h*5, dim_h*5)\n",
    "        self.lin2 = Linear(dim_h*5, max_samp)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        # Node embeddings \n",
    "        h1 = self.conv1(x, edge_index, edge_attr=edge_weight)\n",
    "        h2 = self.conv2(h1, edge_index, edge_attr=edge_weight)\n",
    "        h3 = self.conv3(h2, edge_index, edge_attr=edge_weight)\n",
    "        h4 = self.conv4(h3, edge_index, edge_attr=edge_weight)\n",
    "        h5 = self.conv5(h4, edge_index, edge_attr=edge_weight)\n",
    "        \n",
    "        # Graph-level readout\n",
    "        \n",
    "        h1 = global_max_pool(h1, batch)\n",
    "        h2 = global_max_pool(h2, batch)\n",
    "        h3 = global_max_pool(h3, batch)\n",
    "        h4 = global_max_pool(h4, batch)\n",
    "        h5 = global_max_pool(h5, batch)\n",
    "        \n",
    "        # Concatenate graph embeddings\n",
    "        h = torch.cat((h1, h2, h3, h4, h5), dim=1)\n",
    "        \n",
    "        # Classifier\n",
    "        h = self.lin1(h)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ca21f0-8b61-4394-9d6e-d75edf1b7adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "def trainG(model, loader, epoch, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    with tqdm(loader, unit=\"batch\") as tepoch:\n",
    "         for data in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            optimizer.zero_grad()\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            loss_function = CrossEntropyLoss(weight=class_weights.to(device))\n",
    "            #loss_function = FocalLoss()\n",
    "            loss = loss_function(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def testG(model, loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    predicted_categories = []\n",
    "    true_categories = []\n",
    "    with tqdm(loader, unit=\"batch\") as tepoch:\n",
    "        for data in tepoch:\n",
    "      # Iterate in batches over the training/test dataset.\n",
    "            data = data.to(device)\n",
    "            out = model(data)  \n",
    "            pred = out.argmax(dim=1)# Use the class with highest probability.\n",
    "            predicted_categories.append(pred.cpu().detach().numpy())\n",
    "            true_categories.append(data.y.cpu().detach().numpy())\n",
    "            correct += int((pred == data.y).sum())# Check against ground-truth labels.\n",
    "    #print(confusion_matrix(true_categories, predicted_categories))\n",
    "    #print(classification_report(true_categories, predicted_categories))  \n",
    "    return (correct / len(loader.dataset)), true_categories, predicted_categories   # Derive ratio of correct predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f4ea5-9707-4b70-9426-8c2d77abda58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_classification():\n",
    "    \n",
    "    if classif == \"MTF\":\n",
    "        output = create_MTF_graph()\n",
    "    elif classif == \"visual\":\n",
    "        output = create_visual_graph()\n",
    "    elif classif == \"join\"\n",
    "        output = create_join_graph()\n",
    "    torch.manual_seed(6405)\n",
    "    train_size = int(0.8 * len(output))\n",
    "    test_size = len(output) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(output, [train_size, test_size])\n",
    "    loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "    #________________Select model_________________________________________________\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model = GINE(32).double().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "    loss_plot = np.empty([range_epoch], dtype='float64')\n",
    "    acc_plot = np.zeros([range_epoch], dtype='float64')\n",
    "    best_score = 0\n",
    "    for epoch in range(range_epoch):\n",
    "        result = trainG(model, loader, epoch, optimizer, device)\n",
    "        loss_plot[epoch] = result\n",
    "        if epoch >= epoch_min:\n",
    "            if epoch%epoch_interval == 0: #or epoch == range_epoch:\n",
    "                score, true_categories, predicted_categories = testG(model, DataLoader(test_dataset, batch_size = 1), device)\n",
    "                acc_plot[epoch] = score\n",
    "                if best_score < score:\n",
    "                    best_score = score\n",
    "                    best_tcs = true_categories\n",
    "                    best_pc = predicted_categories\n",
    "                    best_e = epoch\n",
    "    \n",
    "    print('Done!')\n",
    "    \n",
    "    print(confusion_matrix(best_tcs, best_pc))\n",
    "    print(classification_report(best_tcs, best_pc)) \n",
    "    print(best_e)\n",
    "    return loss_plot, acc_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0ae87-871a-4efc-8f3a-b5d957faa9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFileName = 'SynD_data_120.npy'\n",
    "labelsFileName = 'SynD_labels_120.npy'\n",
    "\n",
    "classif = \"MTF\"\n",
    "n_bins = 150\n",
    "cut = 0\n",
    "\n",
    "batch_size = 64*2\n",
    "range_epoch = 4000\n",
    "epoch_min = 100\n",
    "epoch_interval = 5\n",
    "\n",
    "loss_plot, acc_plot = graph_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95771e1e-aca1-43db-a48a-d1f30407d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "f.set_figwidth(20)\n",
    "f.set_figheight(10)\n",
    "\n",
    "plt.plot(loss_plot, '-yo')# main visual, FocalLoss, bin = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e176ad3d-7bbd-45c2-9d0b-39480aa0c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "f.set_figwidth(20)\n",
    "f.set_figheight(10)\n",
    "plt.plot(acc_plot, '-yo',linewidth=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
